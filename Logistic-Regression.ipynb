{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回归问题与分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 问题类型 | 回归 | 分类 |\n",
    "| ------ | ------ | ------ |\n",
    "|  因变量 | 连续的 | 离散的 |\n",
    "| 模型 | 线性回归，支持向量回归，朴素贝叶斯，神经网络 | 逻辑回归，支持向量机，朴素贝叶斯，神经网络 |\n",
    "| 应用场景 | 预测价格、销量、利率、天气（温度、湿度） | 检测垃圾邮件、诈骗、疾病，文章分类 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ h_\\theta (x) = \\frac{1}{1+e^{-x}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1 / ( 1 + np.exp(-x)))\n",
    "\n",
    "x_sig = np.arange(-10, 10, 0.1)\n",
    "plt.plot(x_sig, sigmoid(x_sig))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单位阶跃函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "f(x)=\\left\\{\n",
    "\\begin{aligned}\n",
    "0 (x < 0)  \\\\\n",
    "1 (x \\geq 0) \\\\\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_func(x):\n",
    "   return np.array(x) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_step_func(scale=[-10,10], step=0.01):\n",
    "    x_min, x_max = (scale[0], scale[1])\n",
    "    x_step_left = np.arange(x_min, -0.01, 0.01)\n",
    "    x_step_right = np.arange(0.01, x_max, 0.01)\n",
    "\n",
    "    plt.plot(x_step_left, step_func(x_step_left), color='red')\n",
    "    plt.plot(x_step_right, step_func(x_step_right), color='red')\n",
    "    plt.scatter([0], 0, color='red', facecolors='none', edgecolors='red', s=60)\n",
    "    plt.scatter([0], 1, color='red', facecolors='red', edgecolors='red', s=60)\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_step_func([-15,15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实例演示1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成随机数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRandomData(m, x_scale=(0, 1), noise_factor=0.4):\n",
    "    (x_min, x_max) = x_scale\n",
    "    x_range = x_max - x_min # max value of x\n",
    "    x = np.random.random(size=(m, 2))\n",
    "    noise = (np.random.random(size=(m, 1)) - 0.5 ) * x_range * noise_factor\n",
    "    y = (sigmoid(x[:,0].reshape(m ,1) + x[:,1].reshape(m ,1) + noise - x_range) > 0.5).astype('int')\n",
    "    X = np.c_[np.ones(m), x]\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100 # number of sample data\n",
    "X, y = generateRandomData(m)\n",
    "print(sum(y.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对数据进行可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(X, y, xlabel='x_1', ylabel='x_2', axes=None, label_pos='positive', label_neg='negative'):\n",
    "    pos_index = y[:, 0] == 1\n",
    "    neg_index = y[:, 0] == 0\n",
    "    plt.figure(figsize=(6,6), dpi=80)\n",
    "    if not axes:\n",
    "        axes = plt.gca()\n",
    "        \n",
    "    axes.scatter(X[pos_index][:,1], X[pos_index][:,2], marker='+', c='r', label=label_pos)\n",
    "    axes.scatter(X[neg_index][:,1], X[neg_index][:,2], marker='*', c='b', label=label_neg)\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.grid()\n",
    "    axes.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逻辑回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 假设函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\hat{y} = h_\\theta (x) = g(\\theta^T x)  $   \n",
    "$ g(z)= \\frac{1}{1+e^{-z}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，g表示sigmoid函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代价函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$  J(\\theta)=\\frac{1}{2m} \\sum_{i=1}^m{ -y^{(i)}log(h_\\theta(x)) - (1-y^{(i)})log(1 - h_\\theta(x)) }  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(theta, X, y):\n",
    "    h = sigmoid(X.dot(theta))\n",
    "    J = -1/m * (np.log(h).T.dot(y) + np.log(1-h).T.dot(1-y))\n",
    "    if np.isnan(J[0]):\n",
    "        return np.inf\n",
    "    return (J[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代价函数的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$  \\frac{\\partial{J}}{\\partial{\\theta}} = \\frac{1}{m}\\sum_{i=1}^m{(h_\\theta (x^{(i)}) - y^{(i)} )}) \\cdot x_j^{(i)} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推导过程：   \n",
    "<img src=\"./images/LR-gradient-calc.png\" align=\"left\" style=\"width:50%\" />    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costGradient(theta, X, y):\n",
    "    h = sigmoid(X.dot(theta.reshape(-1,1)))\n",
    "    grad = 1/m * X.T.dot(h-y)\n",
    "    return (grad.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练逻辑回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta_init = np.random.random(X.shape[1])\n",
    "theta_init = np.ones(X.shape[1]) * 0.01\n",
    "cost = computeCost(theta_init, X, y)\n",
    "grad = costGradient(theta_init, X, y)\n",
    "print('cost: %s, Gradient: %s, theta: %s' %(cost, grad, theta_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度下降法迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "    # save all cost values over iterations\n",
    "    J_history = np.zeros(num_iters)\n",
    "    theta_history = np.zeros((num_iters, 3))\n",
    "    for iter in range(num_iters):\n",
    "        hypothesis = sigmoid(X.dot(theta))\n",
    "        theta -= alpha / m * costGradient(theta, X, y) # core step of gradient descent\n",
    "        J_history[iter] = computeCost(theta, X, y)\n",
    "        theta_history[iter,:] = theta.reshape(1,3)\n",
    "    return (theta, J_history, theta_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 300\n",
    "num_iters = 500\n",
    "theta_gd, J_history, theta_history = gradientDescent(X, y, theta_init, alpha, num_iters)\n",
    "print(theta_gd)\n",
    "plt.plot(J_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scipy 中的 [minimize](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.optimize.minimize.html) 方法  \n",
    "- [雅可比矩阵](https://zh.wikipedia.org/wiki/%E9%9B%85%E5%8F%AF%E6%AF%94%E7%9F%A9%E9%98%B5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minimize(computeCost, theta_init, args=(X,y), method=None, jac=costGradient, options={'maxiter': 10000})\n",
    "theta_scipy = res.x\n",
    "print(theta_scipy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 验证测试集的准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = generateRandomData(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, x, threshold=0.5):\n",
    "    return (sigmoid(x.dot(theta.T)) >= threshold).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scipy = sum(predict(theta_scipy, x_test) == y_test.flatten()) / y_test.size\n",
    "accuracy_gd = sum(predict(theta_gd, x_test) == y_test.flatten()) / y_test.size\n",
    "print(accuracy_scipy, accuracy_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 决策边界"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta_scipy, theta_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotIteration(num_iters,x_plot_1, x_plot_2):\n",
    "    alpha = 100\n",
    "    theta_init = np.zeros(X.shape[1])\n",
    "    theta_gd_plt, _, _ = gradientDescent(X, y, theta_init, alpha, num_iters)\n",
    "    \n",
    "    def getHypothesis(theta):\n",
    "        return sigmoid(np.c_[np.ones((x_plot_1.ravel().shape[0],1)), x_plot_1.ravel(), x_plot_2.ravel()].dot(theta)).reshape(x_plot_1.shape)\n",
    "\n",
    "    # calculate hypothesis line based on theta value\n",
    "    h_scipy = getHypothesis(theta_scipy)\n",
    "    h_gd = getHypothesis(theta_gd_plt)\n",
    "    \n",
    "    plt.contour(x_plot_1, x_plot_2, h_gd, [0.5], colors='g')   # gradient descent\n",
    "    plt.contour(x_plot_1, x_plot_2, h_scipy, [0.5], colors='k')  # scipy   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData(X, y)\n",
    "x_plot_1, x_plot_2 = np.meshgrid(np.linspace(X[:,1].min(), X[:,1].max()), np.linspace(X[:,2].min(), X[:,2].max()))\n",
    "# num_iters = 10\n",
    "for i in range(20):\n",
    "    plotIteration((4+i), x_plot_1, x_plot_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多项式逻辑回归与正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRadialData(m, radius, noise_factor=0.2):\n",
    "    angle = np.random.random((m,1)) * 2 * np.pi\n",
    "    r_min = radius * 0.3\n",
    "    r = r_min + np.random.random((m,1)) * radius\n",
    "    x = np.c_[np.cos(angle) * r, np.sin(angle) * r]  # coordinates of data\n",
    "    noise_x = (np.random.random((m,1)) - 0.5) * radius * noise_factor\n",
    "    y = (sigmoid(r + noise_x - 0.5*radius - r_min) > 0.5).astype('int')\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunctionReg(theta, reg, *args):\n",
    "    m = y.size\n",
    "    h = sigmoid(XX.dot(theta))\n",
    "    \n",
    "    J = -1*(1/m)*(np.log(h).T.dot(y)+np.log(1-h).T.dot(1-y)) + (reg/(2*m))*np.sum(np.square(theta[1:]))\n",
    "    \n",
    "    if np.isnan(J[0]):\n",
    "        return(np.inf)\n",
    "    return(J[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientReg(theta, reg, *args):\n",
    "    m = y.size\n",
    "    h = sigmoid(XX.dot(theta.reshape(-1,1)))\n",
    "    grad = (1/m)*XX.T.dot(h-y) + (reg/m)*np.r_[[[0]],theta[1:].reshape(-1,1)]\n",
    "    return(grad.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "degree = 6\n",
    "X, y = generateRadialData(m, 1, 0.8)\n",
    "poly = PolynomialFeatures(degree)\n",
    "XX = poly.fit_transform(X)\n",
    "plotData(XX, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比不同正则化系数对于训练结果的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_theta = np.zeros(XX.shape[1])\n",
    "fig, axes = plt.subplots(2,2, sharey = True, figsize=(17,17))\n",
    "\n",
    "for index, reg in enumerate([0.001, 0.02, 0.1, 1]):\n",
    "    \n",
    "    # Optimize costFunctionReg\n",
    "    res2 = minimize(costFunctionReg, initial_theta, args=(reg, XX, y), method=None, jac=gradientReg, options={'maxiter':3000})\n",
    "    # Accuracy\n",
    "    accuracy = 100*sum(predict(res2.x, XX) == y.ravel())/y.size    \n",
    "    print(accuracy)\n",
    "\n",
    "    x1_min, x1_max = X[:,0].min(), X[:,0].max(),\n",
    "    x2_min, x2_max = X[:,1].min(), X[:,1].max(),\n",
    "    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "    h = sigmoid(poly.fit_transform(np.c_[xx1.ravel(), xx2.ravel()]).dot(res2.x))\n",
    "    h = h.reshape(xx1.shape)\n",
    "    \n",
    "    plotData(XX, y, axes=axes.flatten()[index])\n",
    "    \n",
    "    axes.flatten()[index].contour(xx1, xx2, h, [0.5])\n",
    "    axes.flatten()[index].set_title('Train accuracy {}% with Lambda = {}'.format(np.round(accuracy, decimals=2), reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
